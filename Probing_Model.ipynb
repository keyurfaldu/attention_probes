{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(fname):\n",
    "  with open(fname, \"rb\") as f:\n",
    "    dev_data =  pickle.load(f, encoding=\"latin1\")  # add, encoding=\"latin1\") if using python3 and downloaded data\n",
    "    return (dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionProbe(nn.Module):\n",
    "    def __init__(self, glove_path=\"./data/glove/embeddings.pkl\", num_attention_heads=144, non_trainable=False):\n",
    "        super(AttentionProbe, self).__init__()\n",
    "        \n",
    "        weights_matrix = self.load_glove(glove_path)\n",
    "        self.vocab_length, self.vocab_dim = weights_matrix.shape\n",
    "        self.embeddings = nn.Embedding(self.vocab_length, self.vocab_dim)\n",
    "        self.embeddings.load_state_dict({'weight': torch.tensor(weights_matrix)})\n",
    "        if non_trainable:\n",
    "            self.embeddings.weight.requires_grad = False\n",
    "            \n",
    "        self.weight_layer = nn.Linear(self.vocab_dim + self.vocab_dim, num_attention_heads*2)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.loss = nn.NLLLoss()\n",
    "        \n",
    "    def load_glove(self, glove_path):\n",
    "        weights_matrix = load_pickle(glove_path)\n",
    "        return (weights_matrix)\n",
    "        \n",
    "    def forward(self, tokens, labels, attns):\n",
    "        n_words = len(tokens)\n",
    "        tokens = self.embeddings(tokens)\n",
    "        tokens_pairs = torch.cat((tokens.repeat(1,n_words,1).squeeze().view(n_words*n_words, self.vocab_dim), \n",
    "                          tokens.repeat(1,1,n_words).squeeze().view(n_words*n_words, self.vocab_dim)), \n",
    "                                 dim=-1).view(n_words,n_words,self.vocab_dim+self.vocab_dim).transpose(1,0)\n",
    "        \n",
    "        tokens_pairs = torch.cat((torch.zeros((n_words, 1, 200)).cuda(), tokens_pairs), 1) # dummy for ROOT\n",
    "        #tokens_pairs = torch.cat((tokens_pairs, torch.zeros((n_words, 1, 200)).cuda()), 1) # dummy for ROOT\n",
    "\n",
    "        tokens_h = self.weight_layer(tokens_pairs)\n",
    "        alayers, aheads, awords, awords = attns.shape\n",
    "        #attns = attns.view(alayers*aheads, awords, awords)\n",
    "        attns = torch.cat((attns, attns.transpose(3, 2)), 0).view(alayers*aheads*2, awords, awords)\n",
    "        #attns = attns[:,1:-1, 1:-1]\n",
    "        attns = torch.cat(((attns[:,1:-1, 0] + attns[:,1:-1, -1]).unsqueeze(-1), attns[:,1:-1, 1:-1]),-1)\n",
    "        attns = attns.transpose(2,1).transpose(0,2)\n",
    "        attns_tokens = attns * tokens_h\n",
    "        attns_sum = attns_tokens.sum(-1)\n",
    "        loss = self.loss(self.softmax(attns_sum), labels)\n",
    "        outputs = [loss, attns_sum]\n",
    "        return outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloveTokenizer():\n",
    "    def __init__(self):\n",
    "        self.vocab = load_pickle(\"./data/glove/vocab.pkl\")\n",
    "        \n",
    "    def get_token_ids(self, words):\n",
    "        token_ids = []\n",
    "        for word in words:\n",
    "            token_id = self.vocab.get(word.lower(), 0)\n",
    "            token_ids.append(token_id)\n",
    "        return (torch.tensor(token_ids))\n",
    "\n",
    "tokenizer = GloveTokenizer()\n",
    "model = AttentionProbe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_data = load_pickle(\"./data/ud/ud_attention_data.pkl\")\n",
    "train_samples = round(0.8*len(attention_data))\n",
    "train_data = attention_data[:train_samples]\n",
    "dev_data = attention_data[train_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(example, optimizer):\n",
    "    attns = example[\"attns\"]\n",
    "    labels = torch.tensor(example[\"heads\"]).squeeze()\n",
    "    input_tokens = tokenizer.get_token_ids(example[\"words\"])\n",
    "    input_tokens = input_tokens.cuda()\n",
    "    labels = labels.cuda()\n",
    "    attns = attns.cuda()\n",
    "    outputs = model(input_tokens, labels, attns)\n",
    "    loss = outputs[0]\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    model.zero_grad()\n",
    "    oloss = float(loss.detach().cpu().numpy())\n",
    "    return(oloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3369\n",
      "100/3369\n",
      "200/3369\n",
      "300/3369\n",
      "400/3369\n",
      "500/3369\n",
      "600/3369\n",
      "700/3369\n",
      "800/3369\n",
      "900/3369\n",
      "1000/3369\n",
      "1100/3369\n",
      "1200/3369\n",
      "1300/3369\n",
      "1400/3369\n",
      "1500/3369\n",
      "1600/3369\n",
      "1700/3369\n",
      "1800/3369\n",
      "1900/3369\n",
      "2000/3369\n",
      "2100/3369\n",
      "2200/3369\n",
      "2300/3369\n",
      "2400/3369\n",
      "2500/3369\n",
      "2600/3369\n",
      "2700/3369\n",
      "2800/3369\n",
      "2900/3369\n",
      "3000/3369\n",
      "3100/3369\n",
      "3200/3369\n",
      "3300/3369\n",
      "0/3369\n",
      "100/3369\n",
      "200/3369\n",
      "300/3369\n",
      "400/3369\n",
      "500/3369\n",
      "600/3369\n",
      "700/3369\n",
      "800/3369\n",
      "900/3369\n",
      "1000/3369\n",
      "1100/3369\n",
      "1200/3369\n",
      "1300/3369\n",
      "1400/3369\n",
      "1500/3369\n",
      "1600/3369\n",
      "1700/3369\n",
      "1800/3369\n",
      "1900/3369\n",
      "2000/3369\n",
      "2100/3369\n",
      "2200/3369\n",
      "2300/3369\n",
      "2400/3369\n",
      "2500/3369\n",
      "2600/3369\n",
      "2700/3369\n",
      "2800/3369\n",
      "2900/3369\n",
      "3000/3369\n",
      "3100/3369\n",
      "3200/3369\n",
      "3300/3369\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "learning_rate = 0.002\n",
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    \n",
    "model.cuda()\n",
    "model.train()\n",
    "for epoch in range(2):\n",
    "    for i, example in enumerate(train_data):\n",
    "        if i % 100 == 0:\n",
    "            print(\"{:}/{:}\".format(i, len(train_data)))\n",
    "        if(len(example[\"words\"]) <= 1):\n",
    "            continue\n",
    "        #print(i, example[\"words\"])\n",
    "        loss = train(example, optimizer)\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9759140166319048\n",
      "0.7724338806722153\n",
      "0.9837104285235737\n",
      "0.5255733057877713\n",
      "0.3910336213554157\n",
      "0.562712003280832\n"
     ]
    }
   ],
   "source": [
    "for i in range(round(len(losses)/1000)):\n",
    "    print(np.nanmean(losses[i*1000:((i+1)*1000)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(example):\n",
    "    model.eval()\n",
    "    attns = example[\"attns\"]\n",
    "    labels = torch.tensor(example[\"heads\"]).squeeze()\n",
    "    input_tokens = tokenizer.get_token_ids(example[\"words\"])\n",
    "    input_tokens = input_tokens.cuda()\n",
    "    labels = labels.cuda()\n",
    "    attns = attns.cuda()\n",
    "    outputs = model(input_tokens, labels, attns)\n",
    "    attns_sum = outputs[1]\n",
    "    preds = np.argmax(attns_sum.detach().cpu(), axis=1)\n",
    "    return(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = train_data[6]\n",
    "preds = evaluate(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "0/842\n",
      "100/842\n",
      "200/842\n",
      "300/842\n",
      "400/842\n",
      "500/842\n",
      "600/842\n",
      "700/842\n",
      "800/842\n",
      "UAS: 75.2\n"
     ]
    }
   ],
   "source": [
    "model.cuda()\n",
    "print(\"Evaluating...\")\n",
    "correct, total = 0, 0\n",
    "for i, example in enumerate(dev_data):\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"{:}/{:}\".format(i, len(dev_data)))\n",
    "    if len(example[\"words\"]) <= 1:\n",
    "        continue\n",
    "    preds = evaluate(example)\n",
    "    #print (example[\"words\"])\n",
    "    \n",
    "    for j, (head, prediction, reln) in enumerate(zip(example[\"heads\"], preds.numpy(), example[\"relns\"])):\n",
    "        # it is standard to ignore punct for Stanford Dependency evaluation\n",
    "        #print(head, prediction, reln)\n",
    "        if reln != \"punct\":\n",
    "            if head == prediction:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "print(\"UAS: {:.1f}\".format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medhas01",
   "language": "python",
   "name": "medhas01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
